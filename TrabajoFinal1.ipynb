{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdtiW6P42l3d"
   },
   "source": [
    "Para el desarrollo de este proyecto, se ha seleccionado como fuente de informaci√≥n primaria el archivo SP1.csv (disponible en https://www.football-data.co.uk/spainm.php), el cual contiene el registro detallado de los partidos correspondientes a la Primera Divisi√≥n de Espa√±a (La Liga) para la temporada 2024-2025.\n",
    "\n",
    "Este conjunto de datos nos proporciona una visi√≥n de cada encuentro disputado hasta la fecha. Gracias a la documentaci√≥n t√©cnica consultada (https://www.football-data.co.uk/notes.txt), identificamos que el archivo no solo se limita a los resultados deportivos (goles, victorias/derrotas), sino que estructura la informaci√≥n en tres grandes bloques: datos del encuentro (fecha, equipos), estad√≠sticas de rendimiento (disparos, c√≥rners, faltas, tarjetas) y datos del mercado de apuestas (cuotas de apertura y cierre de m√∫ltiples casas de apuestas)\n",
    "\n",
    "A partir de este conjunto de datos en bruto, procederemos a aplicar las t√©cnicas de preprocesamiento y limpieza necesarias para garantizar la calidad de la informaci√≥n antes de abordar su visualizaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentaci√≥n T√©cnica: Pipeline ETL La Liga 2024-25\n",
    "\n",
    "## Desglose Detallado del Algoritmo (Paso a Paso)\n",
    "\n",
    "### PASO 1: Ingesta de la informaci√≥n primaria (SP1.csv) y Normalizaci√≥n Sem√°ntica\n",
    "\n",
    "**¬øQu√© hace el c√≥digo?**\n",
    "Carga el archivo `SP1.csv` (estad√≠sticas puras del juego). Pero el problema principal es la **heterogeneidad sem√°ntica**.\n",
    "\n",
    "* **Dificultad Encontrada:** Las fuentes de datos no utilizan los mismos nombres de los diferentes equipos:\n",
    "    * El CSV llama al equipo `At Bilbao`.\n",
    "    * Wikipedia lo llama `Athletic Club`.\n",
    "    * EstadiosDB lo llama `Athletic Club de Bilbao`.\n",
    "* **Soluci√≥n T√©cnica:** Se implementa un **Diccionario de Mapeo (`mapa_nombres`)** creado a partir de todos los nombres de los diferentes equipos que se han utilizado en las distintas fuentes de datos.  Antes de hacer cualquier cruce, obligamos a todos los DataFrames a usar una nomenclatura est√°ndar.\n",
    "    * *L√≥gica:* `df.replace(diccionario)` escanea todas las columnas de nombres y unifica las variantes.\n",
    "\n",
    "---\n",
    "\n",
    "### PASO 2: Web Scrapping sobre Wikipedia 1 (PARTIDOS, ASISTENCIA Y ESTADIOS). \n",
    "\n",
    "`SP1.csv` no posee la informaci√≥n de en qu√© estadio se jug√≥ el cada partido ni de los datos de asistencia del mismo. Siendo asi, iteramos sobre 20 URLs (una por equipo) sobre su p√°gina en Wikipedia (√∫nica p√°gina que hemos encontrado que nos daba esta informaci√≥n) para obtener esta informaci√≥n.\n",
    "\n",
    "**Mec√°nica Interna:**\n",
    "1.  **Petici√≥n:** Descargamos el HTML de la temporada 24/25 de cada equipo.\n",
    "2.  **B√∫squeda del Contenedor:** Buscamos etiquetas `div` con la clase `vevent`. En el HTML de Wikipedia, `vevent` es un microformato est√°ndar para \"Eventos\" (partidos).\n",
    "3. **Dificultad A**: Filtrado de Contexto (Ruido en los Datos)\n",
    "    * **El Problema**: Las p√°ginas de Wikipedia mezclan partidos de La Liga, Copa del Rey y Amistosos en una misma lista visual. \n",
    "    * **Soluci√≥n T√©cnica**: B√∫squeda Inversa en el DOM (`find_previous`). Antes de aceptar un partido, el script mira \"hacia arriba\" en el c√≥digo HTML buscando el encabezado (`<h2>` o `<h3>`) anterior m√°s cercano. Solo si ese encabezado contiene el texto \"La Liga\", el script procesa el dato.\n",
    "4. **Dificultad B**: Extracci√≥n de Datos \"Sucios\"\n",
    "    * **El Problema**: El dato de asistencia en el HTML viene sucio, mezclado con texto y referencias bibliogr√°ficas. Ejemplo: \"Attendance: 45,000 [3]\". Python no puede sumar eso.\n",
    "    * **Soluci√≥n T√©cnica**: Usamos `re.search(r'Attendance:\\s*([\\d,]+)', ...)`. Esta f√≥rmula ignora la palabra \"Attendance\", los espacios y los corchetes [3], extrayendo √∫nicamente los d√≠gitos num√©ricos para convertirlos a enteros (int).\n",
    "5. **Dificultad C**: Estadios Vac√≠os\n",
    "    * **El Problema**: A veces Wikipedia no escribe el nombre del estadio en la celda correspondiente si es el estadio habitual del equipo local, dej√°ndolo vac√≠o o como \"Desconocido\".\n",
    "    * **Soluci√≥n T√©cnica**: Aprendizaje y Relleno. Creamos un diccionario din√°mico durante la ejecuci√≥n. Si el script lee una fila completa donde dice \"Local: Celta -> Estadio: Bala√≠dos\", guarda ese conocimiento. M√°s tarde, si encuentra \"Local: Celta -> Estadio: Desconocido\", usa el conocimiento previo para rellenar el hueco autom√°ticamente.\n",
    "\n",
    "---\n",
    "\n",
    "### PASO 3: Web Scraping EstadiosDB (Asistencia Media)\n",
    "\n",
    "Al no ser Wikipedia una fuente oficial de informaci√≥n, hay partidos en los cuales no se dispone de la asistencia de este. Necesitamos por tanto un validador externo, en este caso EstadiosDB, el cual tiene una tabla resumen de la asistencia media a cada estadio durante la temporada 24-25.\n",
    "\n",
    "**¬øQu√© hace el c√≥digo?**\n",
    "Va a una URL espec√≠fica de EstadiosDB que contiene una tabla resumen (`<table class=\"arttab\">`).\n",
    "\n",
    "* **Dificultad de Formato:** Los n√∫meros en Europa usan puntos para miles (45.000) y en Python/USA usan comas o nada. Adem√°s, a veces tienen asteriscos (45.000*).\n",
    "* **Limpieza:** El script aplica `.replace(\".\", \"\").replace(\"*\", \"\")` para convertir el texto sucio en un n√∫mero entero puro (`int`) que podamos sumar y restar matem√°ticamente.\n",
    "\n",
    "---\n",
    "\n",
    "### PASO 4: Imputaci√≥n Estad√≠stica (Relleno de Huecos)\n",
    "\n",
    "Aqu√≠ resolvemos el problema de los datos faltantes en la asistencia de los partidos.\n",
    "\n",
    "**La L√≥gica Matem√°tica:**\n",
    "No usamos la media simple (que se ver√≠a afectada por los ceros), sino una **proyecci√≥n inversa**.\n",
    "\n",
    "1.  **Recuperamos la Media Oficial (M):** Del Paso 3 (ej: 50.000 espectadores).\n",
    "2.  **Calculamos el Volumen Total Esperado (V_tot):**\n",
    "    > `V_tot = Media Oficial * N√∫mero Total de Partidos Jugados`\n",
    "3.  **Sumamos la Asistencia Real (V_real):** Suma de los partidos que S√ç tienen dato.\n",
    "4.  **Calculamos el D√©ficit (Delta):**\n",
    "    > `Delta = V_tot - V_real`\n",
    "5.  **Reparto:** Ese `Delta` (los espectadores que \"faltan\" para cuadrar la media) se divide equitativamente entre los partidos que tienen 0 asistencia.\n",
    "\n",
    "**Resultado:** Obtenemos un dataset completo donde la suma total es coherente con la realidad oficial del club.\n",
    "\n",
    "---\n",
    "\n",
    "### PASO 5: Geolocalizaci√≥n (Extracci√≥n de Coordenadas)\n",
    "\n",
    "Necesitamos colocar los estadios en el mapa. Para ello, vamos a las p√°ginas individuales de Wikipedia de cada estadio (ej: *Santiago Bernab√©u*).\n",
    "\n",
    "**Mec√°nica de Scraping:**\n",
    "1.  **Detecci√≥n de Nombre Oficial:** A veces el nombre coloquial (\"Bernab√©u\") no es el oficial. Buscamos en la `infobox` (la tabla gris a la derecha en Wikipedia) el campo \"Nombre completo\".\n",
    "2.  **Extracci√≥n Geoespacial:**\n",
    "    * Wikipedia oculta las coordenadas para m√°quinas dentro de una etiqueta `<span class=\"geo\">`.\n",
    "    * *Ejemplo de HTML:* `<span class=\"geo\">40.453; -3.688</span>`\n",
    "    * *Dificultad:* A veces usan punto y coma (`;`) como separador, y a veces coma (`,`).\n",
    "    * *Soluci√≥n:* El c√≥digo tiene un `if/else` para detectar qu√© separador se est√° usando, hace un `split`, y asigna Latitud (`[0]`) y Longitud (`[1]`).\n",
    "\n",
    "---\n",
    "\n",
    "### PASO 6: Guardado y Estructura Final\n",
    "\n",
    "El script finaliza volcando la memoria RAM al disco duro. Se generan 3 archivos CSV clave para garantizar la integridad referencial:\n",
    "\n",
    "1.  **`SP1_Normalizado.csv`**: La base de datos deportiva limpia.\n",
    "2.  **`datos_partidos_asistencia.csv`**: La tabla de hechos (Fact Table) con el calendario y el p√∫blico.\n",
    "3.  **`datos_coordenadas.csv`**: La tabla de dimensiones (Dim Table) con la ubicaci√≥n f√≠sica.\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen de Dificultades T√©cnicas Superadas\n",
    "\n",
    "| Dificultad | Soluci√≥n Aplicada en C√≥digo |\n",
    "| :--- | :--- |\n",
    "| **Bloqueo de Bots** | Inyecci√≥n de cabeceras `User-Agent` falsas. |\n",
    "| **Nombres Dispares** | Diccionarios maestros de normalizaci√≥n (`mapa_nombres`). |\n",
    "| **Datos Sucios** | Limpieza con Regex y `.replace` (quitar comas, asteriscos, citas). |\n",
    "| **Estadios Vac√≠os** | L√≥gica de inferencia basada en el equipo Local. |\n",
    "| **Ceros en Asistencia** | Algoritmo matem√°tico de imputaci√≥n basado en la media oficial. |\n",
    "| **Coordenadas Raras** | Parsers condicionales para distintos formatos de separadores GPS. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit==1.52.2 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (1.52.2)\n",
      "Collecting folium>=0.15.0\n",
      "  Downloading folium-0.20.0-py2.py3-none-any.whl (113 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting streamlit-folium>=0.15.0\n",
      "  Downloading streamlit_folium-0.26.1-py3-none-any.whl (523 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m523.7/523.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas==2.3.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (2.3.3)\n",
      "Requirement already satisfied: numpy==2.4.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (2.4.0)\n",
      "Requirement already satisfied: lxml>=5.0.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (6.0.2)\n",
      "Requirement already satisfied: openpyxl>=3.1.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (3.1.5)\n",
      "Requirement already satisfied: plotly==6.5.1 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (6.5.1)\n",
      "Requirement already satisfied: matplotlib==3.10.8 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (3.10.8)\n",
      "Requirement already satisfied: seaborn==0.13.2 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn==1.8.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (1.8.0)\n",
      "Requirement already satisfied: scipy==1.16.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (1.16.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (4.14.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (2.32.5)\n",
      "Requirement already satisfied: requests-cache>=1.1.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (1.2.1)\n",
      "Requirement already satisfied: retry-requests>=2.0.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (2.0.0)\n",
      "Requirement already satisfied: urllib3>=2.0.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 29)) (2.6.3)\n",
      "Requirement already satisfied: openmeteo-requests>=1.1.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (1.7.4)\n",
      "Requirement already satisfied: pytrends>=4.9.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (4.9.2)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (6.0.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (6.2.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (8.3.1)\n",
      "Requirement already satisfied: packaging>=20 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (25.0)\n",
      "Requirement already satisfied: pillow<13,>=7.1.0 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (12.1.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (6.33.2)\n",
      "Requirement already satisfied: pyarrow>=7.0 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (22.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (3.1.46)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in ./venv/lib/python3.11/site-packages (from streamlit==1.52.2->-r requirements.txt (line 5)) (6.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas==2.3.3->-r requirements.txt (line 10)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas==2.3.3->-r requirements.txt (line 10)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas==2.3.3->-r requirements.txt (line 10)) (2025.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./venv/lib/python3.11/site-packages (from plotly==6.5.1->-r requirements.txt (line 16)) (2.15.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.11/site-packages (from matplotlib==3.10.8->-r requirements.txt (line 17)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.11/site-packages (from matplotlib==3.10.8->-r requirements.txt (line 17)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.11/site-packages (from matplotlib==3.10.8->-r requirements.txt (line 17)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.11/site-packages (from matplotlib==3.10.8->-r requirements.txt (line 17)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.11/site-packages (from matplotlib==3.10.8->-r requirements.txt (line 17)) (3.3.1)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./venv/lib/python3.11/site-packages (from scikit-learn==1.8.0->-r requirements.txt (line 21)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./venv/lib/python3.11/site-packages (from scikit-learn==1.8.0->-r requirements.txt (line 21)) (3.6.0)\n",
      "Collecting branca>=0.6.0\n",
      "  Downloading branca-0.8.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: jinja2>=2.9 in ./venv/lib/python3.11/site-packages (from folium>=0.15.0->-r requirements.txt (line 6)) (3.1.6)\n",
      "Collecting xyzservices\n",
      "  Downloading xyzservices-2025.11.0-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: et-xmlfile in ./venv/lib/python3.11/site-packages (from openpyxl>=3.1.0->-r requirements.txt (line 13)) (2.0.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in ./venv/lib/python3.11/site-packages (from beautifulsoup4>=4.12.0->-r requirements.txt (line 25)) (2.8.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.31.0->-r requirements.txt (line 26)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.31.0->-r requirements.txt (line 26)) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests>=2.31.0->-r requirements.txt (line 26)) (2026.1.4)\n",
      "Requirement already satisfied: attrs>=21.2 in ./venv/lib/python3.11/site-packages (from requests-cache>=1.1.0->-r requirements.txt (line 27)) (25.4.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in ./venv/lib/python3.11/site-packages (from requests-cache>=1.1.0->-r requirements.txt (line 27)) (25.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./venv/lib/python3.11/site-packages (from requests-cache>=1.1.0->-r requirements.txt (line 27)) (4.5.1)\n",
      "Requirement already satisfied: url-normalize>=1.4 in ./venv/lib/python3.11/site-packages (from requests-cache>=1.1.0->-r requirements.txt (line 27)) (2.2.1)\n",
      "Requirement already satisfied: niquests>=3.15.2 in ./venv/lib/python3.11/site-packages (from openmeteo-requests>=1.1.0->-r requirements.txt (line 32)) (3.16.1)\n",
      "Requirement already satisfied: openmeteo-sdk>=1.22.0 in ./venv/lib/python3.11/site-packages (from openmeteo-requests>=1.1.0->-r requirements.txt (line 32)) (1.23.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in ./venv/lib/python3.11/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.2->-r requirements.txt (line 5)) (4.26.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.52.2->-r requirements.txt (line 5)) (4.0.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2>=2.9->folium>=0.15.0->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: urllib3-future<3,>=2.13.903 in ./venv/lib/python3.11/site-packages (from niquests>=3.15.2->openmeteo-requests>=1.1.0->-r requirements.txt (line 32)) (2.15.901)\n",
      "Requirement already satisfied: wassima<3,>=1.0.1 in ./venv/lib/python3.11/site-packages (from niquests>=3.15.2->openmeteo-requests>=1.1.0->-r requirements.txt (line 32)) (2.0.3)\n",
      "Requirement already satisfied: flatbuffers==25.9.23 in ./venv/lib/python3.11/site-packages (from openmeteo-sdk>=1.22.0->openmeteo-requests>=1.1.0->-r requirements.txt (line 32)) (25.9.23)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.3.3->-r requirements.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.52.2->-r requirements.txt (line 5)) (5.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.2->-r requirements.txt (line 5)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.2->-r requirements.txt (line 5)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in ./venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit==1.52.2->-r requirements.txt (line 5)) (0.30.0)\n",
      "Requirement already satisfied: h11<1.0.0,>=0.11.0 in ./venv/lib/python3.11/site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests>=1.1.0->-r requirements.txt (line 32)) (0.16.0)\n",
      "Requirement already satisfied: jh2<6.0.0,>=5.0.3 in ./venv/lib/python3.11/site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests>=1.1.0->-r requirements.txt (line 32)) (5.0.10)\n",
      "Requirement already satisfied: qh3<2.0.0,>=1.5.4 in ./venv/lib/python3.11/site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests>=1.1.0->-r requirements.txt (line 32)) (1.5.6)\n",
      "Installing collected packages: xyzservices, branca, folium, streamlit-folium\n",
      "Successfully installed branca-0.8.2 folium-0.20.0 streamlit-folium-0.26.1 xyzservices-2025.11.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "--- INICIO DEL PROCESO MAESTRO DE DATOS ---\n",
      "============================================\n",
      "[INFO] Cargando librer√≠as y configuraciones iniciales...\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 1: Cargando y Normalizando archivo base (SP1.csv)\n",
      "-------------------------------------------------------------\n",
      "   [OK] Archivo cargado. Filas totales: 380\n",
      "   [...] Normalizando nombres de equipos en el CSV base...\n",
      "   [GUARDADO] Archivo 'SP1_Normalizado.csv' listo.\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 2: Extrayendo datos de PARTIDOS desde Wikipedia\n",
      "-------------------------------------------------------------\n",
      "   > Procesando equipo 1/20: 2024%E2%80%9325_Athletic_Bilbao_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Athletic_Bilbao_season\n",
      "   > Procesando equipo 2/20: 2024%E2%80%9325_Atl%C3%A9tico_Madrid_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Atl%C3%A9tico_Madrid_season\n",
      "   > Procesando equipo 3/20: 2024%E2%80%9325_FC_Barcelona_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_FC_Barcelona_season\n",
      "   > Procesando equipo 4/20: 2024%E2%80%9325_RC_Celta_de_Vigo_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_RC_Celta_de_Vigo_season\n",
      "   > Procesando equipo 5/20: 2024%E2%80%9325_Deportivo_Alav%C3%A9s_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Deportivo_Alav%C3%A9s_season\n",
      "   > Procesando equipo 6/20: 2024%E2%80%9325_RCD_Espanyol_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_RCD_Espanyol_season\n",
      "   > Procesando equipo 7/20: 2024%E2%80%9325_Getafe_CF_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Getafe_CF_season\n",
      "   > Procesando equipo 8/20: 2024%E2%80%9325_Girona_FC_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Girona_FC_season\n",
      "   > Procesando equipo 9/20: 2024%E2%80%9325_UD_Las_Palmas_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_UD_Las_Palmas_season\n",
      "   > Procesando equipo 10/20: 2024%E2%80%9325_CD_Legan%C3%A9s_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_CD_Legan%C3%A9s_season\n",
      "   > Procesando equipo 11/20: 2024%E2%80%9325_RCD_Mallorca_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_RCD_Mallorca_season\n",
      "   > Procesando equipo 12/20: 2024%E2%80%9325_CA_Osasuna_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_CA_Osasuna_season\n",
      "   > Procesando equipo 13/20: 2024%E2%80%9325_Rayo_Vallecano_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Rayo_Vallecano_season\n",
      "   > Procesando equipo 14/20: 2024%E2%80%9325_Real_Betis_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Real_Betis_season\n",
      "   > Procesando equipo 15/20: 2024%E2%80%9325_Real_Madrid_CF_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Real_Madrid_CF_season\n",
      "   > Procesando equipo 16/20: 2024%E2%80%9325_Real_Sociedad_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Real_Sociedad_season\n",
      "   > Procesando equipo 17/20: 2024%E2%80%9325_Sevilla_FC_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Sevilla_FC_season\n",
      "   > Procesando equipo 18/20: 2024%E2%80%9325_Valencia_CF_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Valencia_CF_season\n",
      "   > Procesando equipo 19/20: 2024%E2%80%9325_Real_Valladolid_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Real_Valladolid_season\n",
      "   > Procesando equipo 20/20: 2024%E2%80%9325_Villarreal_CF_season\n",
      "   [ERROR] Fallo al leer URL: https://en.wikipedia.org/wiki/2024%E2%80%9325_Villarreal_CF_season\n",
      "   [...] Normalizando nombres extra√≠dos de Wikipedia...\n",
      "   [INFO] Total partidos brutos encontrados: 0\n",
      "   [...] Rellenando estadios 'Desconocido' bas√°ndonos en el equipo local...\n",
      "   [RESULTADO] Total partidos finales tras limpieza: 0\n",
      "   [AVISO] Faltan partidos o el calendario a√∫n no ha terminado (0 partidos).\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 3: Extrayendo ASISTENCIA MEDIA de EstadiosDB\n",
      "-------------------------------------------------------------\n",
      "   > Conectando a: https://estadiosdb.com/noticias/2025/06/espana_asistencia_a_los_estadios_de_la_liga_en_la_temporada_202425\n",
      "   [OK] Datos de EstadiosDB extra√≠dos correctamente.\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 4: Rellenando huecos de asistencia (< 1000 espectadores)\n",
      "-------------------------------------------------------------\n",
      "   [FIN] Total de partidos corregidos: 0\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 5: Obteniendo Latitud/Longitud y Nombres Oficiales\n",
      "-------------------------------------------------------------\n",
      "   > Geolocalizando estadio 1/20: Estadio de Mendizorroza...\n",
      "   > Geolocalizando estadio 2/20: Estadio de San Mam%C3%A9s...\n",
      "   > Geolocalizando estadio 3/20: Estadio Metropolitano (Madrid)...\n",
      "   > Geolocalizando estadio 4/20: Estadio Ol%C3%ADmpico Llu%C3%ADs Companys...\n",
      "   > Geolocalizando estadio 5/20: Estadio de Bala%C3%ADdos...\n",
      "   > Geolocalizando estadio 6/20: RCDE Stadium...\n",
      "   > Geolocalizando estadio 7/20: Coliseum (Getafe)...\n",
      "   > Geolocalizando estadio 8/20: Estadio Municipal de Montilivi...\n",
      "   > Geolocalizando estadio 9/20: Estadio de Gran Canaria...\n",
      "   > Geolocalizando estadio 10/20: Estadio Municipal de Butarque...\n",
      "   > Geolocalizando estadio 11/20: Estadio de Son Moix...\n",
      "   > Geolocalizando estadio 12/20: Estadio El Sadar...\n",
      "   > Geolocalizando estadio 13/20: Estadio de Vallecas...\n",
      "   > Geolocalizando estadio 14/20: Estadio Benito Villamar%C3%ADn...\n",
      "   > Geolocalizando estadio 15/20: Estadio Santiago Bernab%C3%A9u...\n",
      "   > Geolocalizando estadio 16/20: Estadio de Anoeta...\n",
      "   > Geolocalizando estadio 17/20: Estadio Jos%C3%A9 Zorrilla...\n",
      "   > Geolocalizando estadio 18/20: Estadio Ram%C3%B3n S%C3%A1nchez-Pizju%C3%A1n...\n",
      "   > Geolocalizando estadio 19/20: Estadio de Mestalla...\n",
      "   > Geolocalizando estadio 20/20: Estadio de la Cer%C3%A1mica...\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 6: Guardando resultados en disco\n",
      "-------------------------------------------------------------\n",
      "‚úÖ PROCESO COMPLETADO EXITOSAMENTE.\n",
      "üìÇ Archivos generados en: outputs\n",
      "   1. SP1_Normalizado.csv\n",
      "   2. datos_coordenadas.csv\n",
      "   3. datos_partidos_asistencia.csv\n",
      "   4. datos_asistencia_media_estadios.csv\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PROYECTO: EXTRACCI√ìN Y LIMPIEZA DE DATOS DE LALIGA 2024-25\n",
    "# DESCRIPCI√ìN: Script maestro para unificar estad√≠sticas, asistencia y geolocalizaci√≥n.\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. IMPORTACI√ìN DE LIBRER√çAS\n",
    "# ==============================================================================\n",
    "# Importamos las herramientas necesarias:\n",
    "# 'os': Para manejar rutas de archivos y crear carpetas en el sistema operativo.\n",
    "# 're': Para usar expresiones regulares (b√∫squeda de patrones de texto complejos).\n",
    "# 'time': Para pausar la ejecuci√≥n (sleep) y no ser bloqueados por los servidores web.\n",
    "# 'io': Para manejo de flujos de entrada/salida (aunque en este script se usa poco).\n",
    "# 'pandas': La herramienta principal para manipular tablas de datos (DataFrames).\n",
    "# 'requests' y 'urllib': Para realizar peticiones a p√°ginas web (descargar el HTML).\n",
    "# 'BeautifulSoup': Para leer (\"parsear\") y extraer informaci√≥n espec√≠fica del HTML descargado.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "print(\"============================================\")\n",
    "print(\"--- INICIO DEL PROCESO MAESTRO DE DATOS ---\")\n",
    "print(\"============================================\")\n",
    "print(\"[INFO] Cargando librer√≠as y configuraciones iniciales...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURACI√ìN DEL ENTORNO Y RUTAS\n",
    "# ==============================================================================\n",
    "# Definimos d√≥nde est√°n los archivos y d√≥nde se guardar√°n los resultados.\n",
    "\n",
    "# Ruta donde se encuentra tu archivo original de estad√≠sticas (CSV base).\n",
    "RUTA_CSV_ESTADISTICAS = r\"inputs/SP1.csv\"\n",
    "\n",
    "# Carpeta donde guardaremos todos los archivos generados.\n",
    "CARPETA_SALIDA = r\"outputs\"\n",
    "# Verificamos si la carpeta de salida existe; si no, la creamos autom√°ticamente.\n",
    "if not os.path.exists(CARPETA_SALIDA):\n",
    "    os.makedirs(CARPETA_SALIDA)\n",
    "    print(f\"[INFO] Carpeta creada: {CARPETA_SALIDA}\")\n",
    "\n",
    "# Configuraci√≥n de 'User-Agent':\n",
    "# Esto es una \"m√°scara\" para que Wikipedia y otras webs crean que somos un navegador Chrome real\n",
    "# y no un robot (script), evitando que nos bloqueen la conexi√≥n.\n",
    "HEADERS_REQUEST = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DICCIONARIOS MAESTROS DE NORMALIZACI√ìN\n",
    "# ==============================================================================\n",
    "# Estos diccionarios act√∫an como traductores. Aseguran que \"Real Madrid\", \"Real Madrid CF\" \n",
    "# y \"R. Madrid\" sean siempre \"Real Madrid\". Esto es VITAL para poder cruzar las tablas despu√©s.\n",
    "\n",
    "# MAPA DE EQUIPOS (Normalizaci√≥n de nombres)\n",
    "mapa_nombres = {\n",
    "    # DEPORTIVO ALAV√âS\n",
    "    'Alaves': 'Deportivo Alav√©s', \n",
    "    'Alav√©s': 'Deportivo Alav√©s', \n",
    "    'Deportivo Alav√©s': 'Deportivo Alav√©s',\n",
    "    # ATHLETIC CLUB\n",
    "    'Ath Bilbao': 'Athletic Club', \n",
    "    'Athletic Bilbao': 'Athletic Club', \n",
    "    'Bilbao': 'Athletic Club', \n",
    "    'Athletic Club': 'Athletic Club',\n",
    "    # ATL√âTICO DE MADRID\n",
    "    'Ath Madrid': 'Atl√©tico de Madrid', \n",
    "    'Atl√©tico Madrid': 'Atl√©tico de Madrid', \n",
    "    'Atl√©tico de Madrid': 'Atl√©tico de Madrid',\n",
    "    # FC BARCELONA\n",
    "    'Barcelona': 'FC Barcelona', \n",
    "    'FC Barcelona': 'FC Barcelona',\n",
    "    # REAL BETIS\n",
    "    'Betis': 'Real Betis', \n",
    "    'Real Betis': 'Real Betis',\n",
    "    # RC CELTA\n",
    "    'Celta': 'RC Celta', \n",
    "    'Celta Vigo': 'RC Celta', \n",
    "    'Celta de Vigo': 'RC Celta', \n",
    "    'RC Celta de Vigo': 'RC Celta', \n",
    "    'RC Celta': 'RC Celta',\n",
    "    # RCD ESPANYOL\n",
    "    'Espanol': 'RCD Espanyol', \n",
    "    'Espanyol': 'RCD Espanyol', \n",
    "    'RCD Espanyol': 'RCD Espanyol',\n",
    "    # GETAFE CF\n",
    "    'Getafe': 'Getafe CF', \n",
    "    'Getafe CF': 'Getafe CF',\n",
    "    # GIRONA FC\n",
    "    'Girona': 'Girona FC', \n",
    "    'Girona FC': 'Girona FC',\n",
    "    # UD LAS PALMAS\n",
    "    'Las Palmas': 'UD Las Palmas', \n",
    "    'UD Las Palmas': 'UD Las Palmas',\n",
    "    # CD LEGAN√âS\n",
    "    'Leganes': 'CD Legan√©s', \n",
    "    'Legan√©s': 'CD Legan√©s', \n",
    "    'CD Legan√©s': 'CD Legan√©s',\n",
    "    # RCD MALLORCA\n",
    "    'Mallorca': 'RCD Mallorca', \n",
    "    'RCD Mallorca': 'RCD Mallorca',\n",
    "    # CA OSASUNA\n",
    "    'Osasuna': 'CA Osasuna', \n",
    "    'CA Osasuna': 'CA Osasuna',\n",
    "    # RAYO VALLECANO\n",
    "    'Vallecano': 'Rayo Vallecano', \n",
    "    'Rayo Vallecano': 'Rayo Vallecano',\n",
    "    # REAL MADRID\n",
    "    'Real Madrid': 'Real Madrid', \n",
    "    'Real Madrid CF': 'Real Madrid',\n",
    "    # REAL SOCIEDAD\n",
    "    'Sociedad': 'Real Sociedad', \n",
    "    'Real Sociedad': 'Real Sociedad',\n",
    "    # SEVILLA FC\n",
    "    'Sevilla': 'Sevilla FC', \n",
    "    'Sevilla FC': 'Sevilla FC',\n",
    "    # VALENCIA CF\n",
    "    'Valencia': 'Valencia CF', \n",
    "    'Valencia CF': 'Valencia CF',\n",
    "    # REAL VALLADOLID\n",
    "    'Valladolid': 'Real Valladolid', \n",
    "    'Real Valladolid': 'Real Valladolid',\n",
    "    # VILLARREAL CF\n",
    "    'Villarreal': 'Villarreal CF', \n",
    "    'Villarreal CF': 'Villarreal CF'\n",
    "}\n",
    "\n",
    "# MAPA DE ESTADIOS (Normalizaci√≥n de nombres de estadios)\n",
    "mapa_estadios = {\n",
    "    # DEPORTIVO ALAV√âS\n",
    "    'Mendizorrotza': 'Estadio de Mendizorroza', \n",
    "    'Mendizorrotza Stadium': 'Estadio de Mendizorroza', \n",
    "    'Estadio de Mendizorroza': 'Estadio de Mendizorroza', \n",
    "    'Mendizorroza': 'Estadio de Mendizorroza',\n",
    "    # ATHLETIC CLUB\n",
    "    'San Mam√©s': 'Estadio de San Mam√©s', \n",
    "    'Estadio San Mam√©s': 'Estadio de San Mam√©s', \n",
    "    'Estadio de San Mam√©s': 'Estadio de San Mam√©s',\n",
    "    # ATL√âTICO DE MADRID\n",
    "    'C√≠vitas Metropolitano': 'Estadio Metropolitano', \n",
    "    'Riyadh Air Metropolitano': 'Estadio Metropolitano', \n",
    "    'Estadio Riyadh Air Metropolitano': 'Estadio Metropolitano', \n",
    "    'Metropolitano': 'Estadio Metropolitano', \n",
    "    'Metropolitano Stadium': 'Estadio Metropolitano', \n",
    "    'Estadio Metropolitano': 'Estadio Metropolitano',\n",
    "    # FC BARCELONA\n",
    "    'Estadi Ol√≠mpic Llu√≠s Companys': 'Estadi Ol√≠mpic Llu√≠s Companys', \n",
    "    'Llu√≠s Companys Olympic Stadium': 'Estadi Ol√≠mpic Llu√≠s Companys', \n",
    "    'Estadio Ol√≠mpico Llu√≠s Companys': 'Estadi Ol√≠mpic Llu√≠s Companys', \n",
    "    'Ol√≠mpic Llu√≠s Companys': 'Estadi Ol√≠mpic Llu√≠s Companys',\n",
    "    # REAL BETIS\n",
    "    'Benito Villamar√≠n': 'Estadio Benito Villamar√≠n', \n",
    "    'Estadio Benito Villamar√≠n': 'Estadio Benito Villamar√≠n',\n",
    "    # RC CELTA\n",
    "    'Bala√≠dos': 'Estadio de Bala√≠dos', \n",
    "    'Abanca-Bala√≠dos': 'Estadio de Bala√≠dos', \n",
    "    'Estadio Abanca Bala√≠dos': 'Estadio de Bala√≠dos', ''\n",
    "    'Estadio de Bala√≠dos': 'Estadio de Bala√≠dos', \n",
    "    'ABANCA Bala√≠dos': 'Estadio de Bala√≠dos',\n",
    "    # RCD ESPANYOL\n",
    "    'RCDE Stadium': 'RCDE Stadium', \n",
    "    'Stage Front Stadium': 'RCDE Stadium',\n",
    "    # GETAFE CF\n",
    "    'Coliseum': 'Estadio Coliseum', \n",
    "    'Coliseum (Getafe)': 'Estadio Coliseum', \n",
    "    'Estadio Coliseum': 'Estadio Coliseum',\n",
    "    # GIRONA FC\n",
    "    'Montilivi': 'Estadi Montilivi', \n",
    "    'Estadi Montilivi': 'Estadi Montilivi', \n",
    "    'Estadio Municipal de Montilivi': 'Estadi Montilivi',\n",
    "    # UD LAS PALMAS\n",
    "    'Gran Canaria': 'Estadio de Gran Canaria', \n",
    "    'Estadio Gran Canaria': 'Estadio de Gran Canaria', \n",
    "    'Estadio de Gran Canaria': 'Estadio de Gran Canaria',\n",
    "    # CD LEGAN√âS\n",
    "    'Butarque': 'Estadio Municipal de Butarque', \n",
    "    'Estadio Municipal Butarque': 'Estadio Municipal de Butarque', \n",
    "    'Estadio Municipal de Butarque': 'Estadio Municipal de Butarque', \n",
    "    'Municipal de Butarque': 'Estadio Municipal de Butarque',\n",
    "    # RCD MALLORCA\n",
    "    'Son Moix': 'Estadi Mallorca Son Moix', \n",
    "    'Mallorca Son Moix': 'Estadi Mallorca Son Moix', \n",
    "    'Estadi Mallorca Son Moix': 'Estadi Mallorca Son Moix', \n",
    "    'Estadio de Son Moix': 'Estadi Mallorca Son Moix', \n",
    "    'Visit Mallorca Estadi': 'Estadi Mallorca Son Moix',\n",
    "    # CA OSASUNA\n",
    "    'El Sadar': 'Estadio El Sadar', \n",
    "    'Estadio El Sadar': 'Estadio El Sadar',\n",
    "    # RAYO VALLECANO\n",
    "    'Vallecas': 'Estadio de Vallecas', \n",
    "    'Estadio de Vallecas': 'Estadio de Vallecas', \n",
    "    'Campo de F√∫tbol de Vallecas': 'Estadio de Vallecas', \n",
    "    'Vallecas Stadium': 'Estadio de Vallecas',\n",
    "    # REAL MADRID\n",
    "    'Santiago Bernab√©u': 'Estadio Santiago Bernab√©u', \n",
    "    'Estadio Santiago Bernab√©u': 'Estadio Santiago Bernab√©u', \n",
    "    'Santiago Bernab√©u Stadium': 'Estadio Santiago Bernab√©u',\n",
    "    # REAL SOCIEDAD\n",
    "    'Anoeta': 'Estadio de Anoeta', \n",
    "    'Anoeta Stadium': 'Estadio de Anoeta', \n",
    "    'Reale Arena': 'Estadio de Anoeta', \n",
    "    'Estadio de Anoeta': 'Estadio de Anoeta',\n",
    "    # SEVILLA FC\n",
    "    'Ram√≥n S√°nchez Pizju√°n': 'Estadio Ram√≥n S√°nchez-Pizju√°n', \n",
    "    'Estadio Ram√≥n S√°nchez-Pizju√°n': 'Estadio Ram√≥n S√°nchez-Pizju√°n', \n",
    "    'Ram√≥n S√°nchez Pizju√°n Stadium': 'Estadio Ram√≥n S√°nchez-Pizju√°n', \n",
    "    'Ram√≥n S√°nchez-Pizju√°n': 'Estadio Ram√≥n S√°nchez-Pizju√°n',\n",
    "    # VALENCIA CF\n",
    "    'Mestalla': 'Estadio de Mestalla', \n",
    "    'Estadio de Mestalla': 'Estadio de Mestalla',\n",
    "    # REAL VALLADOLID\n",
    "    'Jos√© Zorrilla': 'Estadio Jos√© Zorrilla', \n",
    "    'Estadio Jos√© Zorrilla': 'Estadio Jos√© Zorrilla',\n",
    "    # VILLARREAL CF\n",
    "    'La Cer√°mica': 'Estadio de la Cer√°mica', \n",
    "    'Estadio de la Cer√°mica': 'Estadio de la Cer√°mica',\n",
    "    # GEN√âRICOS\n",
    "    'Desconocido': 'Desconocido'\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# PASO 1: CARGA Y NORMALIZACI√ìN DEL ARCHIVO BASE (SP1.csv)\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 1: Cargando y Normalizando archivo base (SP1.csv)\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "if os.path.exists(RUTA_CSV_ESTADISTICAS):\n",
    "    # Leemos el CSV\n",
    "    df_base = pd.read_csv(RUTA_CSV_ESTADISTICAS)\n",
    "    print(f\"   [OK] Archivo cargado. Filas totales: {len(df_base)}\")\n",
    "    \n",
    "    if 'HomeTeam' in df_base.columns and 'AwayTeam' in df_base.columns:\n",
    "        # Normalizamos los nombres de los equipos en el archivo original\n",
    "        print(\"   [...] Normalizando nombres de equipos en el CSV base...\")\n",
    "        df_base['HomeTeam'] = df_base['HomeTeam'].replace(mapa_nombres)\n",
    "        df_base['AwayTeam'] = df_base['AwayTeam'].replace(mapa_nombres)\n",
    "        \n",
    "        # Guardamos una copia limpia\n",
    "        ruta_sp1_norm = os.path.join(CARPETA_SALIDA, \"SP1_Normalizado.csv\")\n",
    "        df_base.to_csv(ruta_sp1_norm, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   [GUARDADO] Archivo 'SP1_Normalizado.csv' listo.\")\n",
    "    else:\n",
    "        print(\"   [ALERTA] No se encontraron columnas 'HomeTeam'/'AwayTeam'.\")\n",
    "else:\n",
    "    print(f\"   [ERROR] No se encuentra el archivo en {RUTA_CSV_ESTADISTICAS}\")\n",
    "    df_base = pd.DataFrame() \n",
    "\n",
    "# ==============================================================================\n",
    "# PASO 2: WEB SCRAPING DE WIKIPEDIA (PARTIDOS, ASISTENCIA Y ESTADIOS)\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 2: Extrayendo datos de PARTIDOS desde Wikipedia\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "# Lista de URLs de las temporadas 2024-25 para cada equipo\n",
    "urls_partidos = [\n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Athletic_Bilbao_season\", \n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Atl%C3%A9tico_Madrid_season\",\n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_FC_Barcelona_season\", \n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_RC_Celta_de_Vigo_season\",\n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Deportivo_Alav%C3%A9s_season\", \n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_RCD_Espanyol_season\",\n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Getafe_CF_season\", \n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Girona_FC_season\",\n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_UD_Las_Palmas_season\", \n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_CD_Legan%C3%A9s_season\",\n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_RCD_Mallorca_season\", \n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_CA_Osasuna_season\",\n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Rayo_Vallecano_season\", \n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Real_Betis_season\",\n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Real_Madrid_CF_season\", \n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Real_Sociedad_season\",\n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Sevilla_FC_season\", \n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Valencia_CF_season\",\n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Real_Valladolid_season\", \n",
    "    \"https://en.wikipedia.org/wiki/2024%E2%80%9325_Villarreal_CF_season\"\n",
    "]\n",
    "\n",
    "all_data_partidos = []\n",
    "\n",
    "# Iteramos sobre cada URL para extraer los partidos\n",
    "for i, url in enumerate(urls_partidos):\n",
    "    # Mostramos progreso en pantalla para saber que no se ha colgado\n",
    "    print(f\"   > Procesando equipo {i+1}/{len(urls_partidos)}: {url.split('wiki/')[1]}\")\n",
    "    \n",
    "    try:\n",
    "        req = Request(url, headers=HEADERS_REQUEST)\n",
    "        html = urlopen(req)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Buscamos las cajas de los partidos (clase 'vevent' en Wikipedia)\n",
    "        matches = soup.find_all(\"div\", {\"class\": \"vevent\"})\n",
    "        \n",
    "        for box in matches:\n",
    "            try:\n",
    "                # Verificamos que sea un partido de \"La Liga\" mirando el encabezado previo\n",
    "                prev_header = box.find_previous([\"h2\", \"h3\"])\n",
    "                if prev_header and \"la liga\" in prev_header.get_text().lower():\n",
    "                    \n",
    "                    teams_spans = box.find_all(\"span\", class_=\"fn org\")\n",
    "                    if len(teams_spans) >= 2:\n",
    "                        home = teams_spans[0].get_text(strip=True)\n",
    "                        away = teams_spans[1].get_text(strip=True)\n",
    "                        \n",
    "                        # Extraemos estadio y asistencia\n",
    "                        stadium = box.find(\"span\", class_=\"location\").get_text(strip=True) if box.find(\"span\", class_=\"location\") else \"Desconocido\"\n",
    "                        \n",
    "                        att_match = re.search(r'Attendance:\\s*([\\d,]+)', box.get_text())\n",
    "                        attendance = int(att_match.group(1).replace(\",\", \"\")) if att_match else 0\n",
    "                        \n",
    "                        all_data_partidos.append([home, away, stadium, attendance])\n",
    "            except: continue\n",
    "    except: \n",
    "        print(f\"   [ERROR] Fallo al leer URL: {url}\")\n",
    "        continue\n",
    "\n",
    "# Creamos el DataFrame con todo lo extra√≠do\n",
    "df_wiki = pd.DataFrame(all_data_partidos, columns=[\"Local\", \"Visitante\", \"Estadio\", \"Asistencia\"])\n",
    "\n",
    "# --- NORMALIZACI√ìN DE SCRAPING ---\n",
    "print(\"   [...] Normalizando nombres extra√≠dos de Wikipedia...\")\n",
    "df_wiki[\"Local\"] = df_wiki[\"Local\"].replace(mapa_nombres)\n",
    "df_wiki[\"Visitante\"] = df_wiki[\"Visitante\"].replace(mapa_nombres)\n",
    "df_wiki[\"Estadio\"] = df_wiki[\"Estadio\"].replace(mapa_estadios)\n",
    "\n",
    "print(f\"   [INFO] Total partidos brutos encontrados: {len(df_wiki)}\")\n",
    "\n",
    "# --- LIMPIEZA Y CORRECCI√ìN DE ESTADIOS ---\n",
    "print(\"   [...] Rellenando estadios 'Desconocido' bas√°ndonos en el equipo local...\")\n",
    "\n",
    "# Filtramos las filas que S√ç tienen estadio conocido\n",
    "df_con_estadio = df_wiki[df_wiki['Estadio'] != 'Desconocido']\n",
    "\n",
    "# Creamos un \"mapa de aprendizaje\": {Equipo Local -> Estadio T√≠pico}\n",
    "mapa_estadios_detectados = df_con_estadio.drop_duplicates(subset=['Local']).set_index('Local')['Estadio'].to_dict()\n",
    "\n",
    "def rellenar_estadio(row):\n",
    "    if row['Estadio'] == 'Desconocido':\n",
    "        return mapa_estadios_detectados.get(row['Local'], 'Desconocido')\n",
    "    return row['Estadio']\n",
    "\n",
    "# Aplicamos la correcci√≥n\n",
    "df_wiki['Estadio'] = df_wiki.apply(rellenar_estadio, axis=1)\n",
    "\n",
    "# --- DEDUPLICACI√ìN FINAL ---\n",
    "# Si hay datos duplicados, nos quedamos con el que tenga mayor asistencia (dato m√°s completo)\n",
    "df_wiki['Asistencia'] = pd.to_numeric(df_wiki['Asistencia'], errors='coerce').fillna(0)\n",
    "df_wiki['tiene_estadio'] = df_wiki['Estadio'] != \"Desconocido\"\n",
    "df_wiki = df_wiki.sort_values(by=['Asistencia', 'tiene_estadio'], ascending=[False, False])\n",
    "df_wiki = df_wiki.drop_duplicates(subset=['Local', 'Visitante'], keep='first')\n",
    "df_wiki = df_wiki.drop(columns=['tiene_estadio']).reset_index(drop=True)\n",
    "\n",
    "print(f\"   [RESULTADO] Total partidos finales tras limpieza: {len(df_wiki)}\")\n",
    "if len(df_wiki) == 380:\n",
    "    print(\"   [√âXITO] Calendario completo (380 partidos).\")\n",
    "else:\n",
    "    print(f\"   [AVISO] Faltan partidos o el calendario a√∫n no ha terminado ({len(df_wiki)} partidos).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PASO 3: WEB SCRAPING ESTADIOSDB (ASISTENCIA MEDIA)\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 3: Extrayendo ASISTENCIA MEDIA de EstadiosDB\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "url_resumen = \"https://estadiosdb.com/noticias/2025/06/espana_asistencia_a_los_estadios_de_la_liga_en_la_temporada_202425\"\n",
    "data_resumen = []\n",
    "\n",
    "try:\n",
    "    print(f\"   > Conectando a: {url_resumen}\")\n",
    "    resp = requests.get(url_resumen, headers=HEADERS_REQUEST)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    table = soup.find(\"table\", class_=\"arttab\")\n",
    "    \n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")[1:] # Saltamos la cabecera\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) >= 6:\n",
    "                club = cols[1].get_text(strip=True)\n",
    "                est = cols[2].get_text(strip=True)\n",
    "                # Limpiamos puntos y asteriscos de los n√∫meros\n",
    "                cap = int(cols[3].get_text(strip=True).replace(\".\", \"\").replace(\"*\", \"\"))\n",
    "                asis_media = int(cols[4].get_text(strip=True).replace(\".\", \"\").replace(\"*\", \"\"))\n",
    "                \n",
    "                data_resumen.append([club, est, asis_media])\n",
    "        print(\"   [OK] Datos de EstadiosDB extra√≠dos correctamente.\")\n",
    "    else:\n",
    "        print(\"   [ERROR] No se encontr√≥ la tabla en EstadiosDB.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   [ERROR CR√çTICO] EstadiosDB: {e}\")\n",
    "\n",
    "df_estadios = pd.DataFrame(data_resumen, columns=[\"Club\", \"Estadio_DB\", \"Asistencia_Media\"])\n",
    "df_estadios[\"Club\"] = df_estadios[\"Club\"].replace(mapa_nombres)\n",
    "\n",
    "# ==============================================================================\n",
    "# PASO 4: IMPUTACI√ìN (RELLENO INTELIGENTE) DE DATOS FALTANTES\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 4: Rellenando huecos de asistencia (< 1000 espectadores)\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "# L√≥gica: Si faltan datos de asistencia en un partido, usamos la media del equipo \n",
    "# ajustada para que la suma total coincida con la realidad aproximada.\n",
    "\n",
    "def es_dato_valido(valor):\n",
    "    return valor >= 1000 \n",
    "\n",
    "count_correcciones = 0\n",
    "\n",
    "for index, row in df_estadios.iterrows():\n",
    "    equipo = row['Club']\n",
    "    media_temporada = row['Asistencia_Media']\n",
    "    \n",
    "    mask_local = df_wiki['Local'] == equipo\n",
    "    partidos_local = df_wiki[mask_local]\n",
    "    \n",
    "    total_partidos = len(partidos_local)\n",
    "    if total_partidos == 0: continue\n",
    "\n",
    "    # Separamos partidos con dato bien vs partidos con dato mal (0 o muy bajo)\n",
    "    partidos_validos = partidos_local[partidos_local['Asistencia'].apply(es_dato_valido)]\n",
    "    partidos_a_corregir = partidos_local[~partidos_local['Asistencia'].apply(es_dato_valido)]\n",
    "    \n",
    "    num_a_corregir = len(partidos_a_corregir)\n",
    "    if num_a_corregir == 0: continue\n",
    "        \n",
    "    # Calculamos cu√°nto falta para llegar a la media te√≥rica\n",
    "    asistencia_total_teorica = media_temporada * total_partidos\n",
    "    asistencia_real_acumulada = partidos_validos['Asistencia'].sum()\n",
    "    asistencia_faltante_total = asistencia_total_teorica - asistencia_real_acumulada\n",
    "    \n",
    "    if asistencia_faltante_total > 0:\n",
    "        nueva_estimacion = int(asistencia_faltante_total / num_a_corregir)\n",
    "    else:\n",
    "        nueva_estimacion = int(media_temporada)\n",
    "\n",
    "    print(f\"   > Correcci√≥n en {equipo}: {num_a_corregir} partidos imputados con valor {nueva_estimacion}\")\n",
    "    \n",
    "    indices_a_corregir = partidos_a_corregir.index\n",
    "    df_wiki.loc[indices_a_corregir, 'Asistencia'] = nueva_estimacion\n",
    "    count_correcciones += num_a_corregir\n",
    "\n",
    "print(f\"   [FIN] Total de partidos corregidos: {count_correcciones}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PASO 5: EXTRACCI√ìN DE COORDENADAS Y NOMBRES OFICIALES\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 5: Obteniendo Latitud/Longitud y Nombres Oficiales\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "urls_estadios = [\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_de_Mendizorroza\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_de_San_Mam%C3%A9s\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_Metropolitano_(Madrid)\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_Ol%C3%ADmpico_Llu%C3%ADs_Companys\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_de_Bala%C3%ADdos\",\n",
    "    \"https://es.wikipedia.org/wiki/RCDE_Stadium\",\n",
    "    \"https://es.wikipedia.org/wiki/Coliseum_(Getafe)\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_Municipal_de_Montilivi\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_de_Gran_Canaria\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_Municipal_de_Butarque\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_de_Son_Moix\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_El_Sadar\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_de_Vallecas\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_Benito_Villamar%C3%ADn\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_Santiago_Bernab%C3%A9u\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_de_Anoeta\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_Jos%C3%A9_Zorrilla\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_Ram%C3%B3n_S%C3%A1nchez-Pizju%C3%A1n\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_de_Mestalla\",\n",
    "    \"https://es.wikipedia.org/wiki/Estadio_de_la_Cer%C3%A1mica\"\n",
    "]\n",
    "\n",
    "data_geo = []\n",
    "\n",
    "for i, url in enumerate(urls_estadios):\n",
    "    # Print de progreso para la geolocalizaci√≥n\n",
    "    nombre_temp = url.split('/')[-1].replace(\"_\", \" \")\n",
    "    print(f\"   > Geolocalizando estadio {i+1}/{len(urls_estadios)}: {nombre_temp}...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS_REQUEST)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"     [ERROR HTTP] C√≥digo {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # --- EXTRACCI√ìN DEL NOMBRE OFICIAL ---\n",
    "        nombre_final = \"Desconocido\"\n",
    "        infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "        \n",
    "        encontrado_en_tabla = False\n",
    "        if infobox:\n",
    "            rows = infobox.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                header = row.find(\"th\")\n",
    "                if header and \"Nombre completo\" in header.get_text(strip=True):\n",
    "                    td = row.find(\"td\")\n",
    "                    if td:\n",
    "                        nombre_final = td.get_text(strip=True)\n",
    "                        encontrado_en_tabla = True\n",
    "                        break\n",
    "        \n",
    "        if not encontrado_en_tabla:\n",
    "            h1 = soup.find(\"h1\", id=\"firstHeading\")\n",
    "            if h1:\n",
    "                nombre_final = h1.get_text(strip=True)\n",
    "\n",
    "        nombre_final = re.sub(r'\\[.*?\\]', '', nombre_final).strip()\n",
    "\n",
    "        # --- EXTRACCI√ìN DE COORDENADAS (GEO) ---\n",
    "        latitud = None\n",
    "        longitud = None\n",
    "        \n",
    "        geo_span = soup.find(\"span\", class_=\"geo\")\n",
    "        \n",
    "        if geo_span:\n",
    "            coord_text = geo_span.get_text(strip=True)\n",
    "            # Manejo de formatos con \";\" o con \",\"\n",
    "            if \";\" in coord_text:\n",
    "                 coords = coord_text.split(\";\") \n",
    "            else:\n",
    "                 coords = coord_text.split(\",\")\n",
    "            \n",
    "            if len(coords) >= 2:\n",
    "                latitud = coords[0].strip()\n",
    "                longitud = coords[1].strip()\n",
    "        \n",
    "        data_geo.append([nombre_final, latitud, longitud])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     [EXCEPCI√ìN] Error procesando {url}: {e}\")\n",
    "    \n",
    "    # Peque√±a pausa para ser \"educados\" con el servidor\n",
    "    time.sleep(0.5)\n",
    "\n",
    "df_geo = pd.DataFrame(data_geo, columns=[\"Estadio_Oficial\", \"Latitud\", \"Longitud\"])\n",
    "df_geo[\"Estadio_Oficial\"] = df_geo[\"Estadio_Oficial\"].replace(mapa_estadios)\n",
    "\n",
    "# ==============================================================================\n",
    "# PASO 6: GUARDADO DE TODOS LOS ARCHIVOS FINALES\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 6: Guardando resultados en disco\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "# 1. Archivo de Coordenadas\n",
    "ruta_geo = os.path.join(CARPETA_SALIDA, \"datos_coordenadas.csv\")\n",
    "df_geo.to_csv(ruta_geo, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 2. Archivo de Partidos y Asistencia (Corregido y con Estadios Rellenados)\n",
    "ruta_partidos = os.path.join(CARPETA_SALIDA, \"datos_partidos_asistencia.csv\")\n",
    "df_wiki.to_csv(ruta_partidos, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 3. Archivo de Capacidad y Asistencia Media (EstadiosDB)\n",
    "ruta_estadios_media = os.path.join(CARPETA_SALIDA, \"datos_asistencia_media_estadios.csv\")\n",
    "df_estadios.to_csv(ruta_estadios_media, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"‚úÖ PROCESO COMPLETADO EXITOSAMENTE.\")\n",
    "print(f\"üìÇ Archivos generados en: {CARPETA_SALIDA}\")\n",
    "print(\"   1. SP1_Normalizado.csv\")\n",
    "print(\"   2. datos_coordenadas.csv\")\n",
    "print(\"   3. datos_partidos_asistencia.csv\")\n",
    "print(\"   4. datos_asistencia_media_estadios.csv\")\n",
    "print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå¶Ô∏è Documentaci√≥n T√©cnica: Enriquecimiento Clim√°tico (Open-Meteo)\n",
    "\n",
    "## üìã 1. Introducci√≥n y Objetivo\n",
    "\n",
    "El objetivo de este script es a√±adir una **Dimensi√≥n Ambiental** al dataset maestro de partidos. No basta con saber *qui√©n* gan√≥ o *cu√°nta* gente fue; queremos saber *en qu√© condiciones* se jug√≥.\n",
    "\n",
    "El script consulta una API meteorol√≥gica hist√≥rica para obtener las condiciones exactas (temperatura, lluvia, viento) en las coordenadas precisas del estadio y a la hora exacta del pitido inicial.\n",
    "\n",
    "**¬øPor qu√© es importante?**\n",
    "Permite responder preguntas de negocio como:\n",
    "* *\"¬øLa lluvia reduce dr√°sticamente la asistencia en estadios descubiertos?\"*\n",
    "* *\"¬øAfecta el calor extremo (>30¬∞C) al n√∫mero de goles marcados?\"*\n",
    "* *\"¬øHay una correlaci√≥n entre sensaci√≥n t√©rmica baja y venta de entradas?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è 2. Arquitectura del Script\n",
    "\n",
    "El flujo de trabajo es lineal y robusto, dise√±ado para manejar fallos de red y l√≠mites de API.\n",
    "\n",
    "1.  **Configuraci√≥n del Cliente API:** Setup de cach√© y reintentos (Retries).\n",
    "2.  **Ingesta de Datos:** Carga del CSV con coordenadas geogr√°ficas (`Latitud`, `Longitud`) y temporales (`Date`, `Time`).\n",
    "3.  **Iteraci√≥n y Consulta:** Bucle `for` que procesa cada partido individualmente.\n",
    "4.  **Extracci√≥n de Precisi√≥n:** Filtrado del dato horario exacto del evento.\n",
    "5.  **Consolidaci√≥n:** Uni√≥n de las nuevas m√©tricas al DataFrame original.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 3. An√°lisis de la Fuente de Datos: Open-Meteo\n",
    "\n",
    "Utilizamos la **Historical Weather API** de Open-Meteo. A diferencia de otras APIs, esta no requiere *API Key* para uso no comercial, pero requiere un manejo cuidadoso de las peticiones para no ser bloqueados.\n",
    "\n",
    "### Par√°metros de la Petici√≥n (Payload)\n",
    "\n",
    "Para cada partido, el script construye una petici√≥n din√°mica con estos par√°metros:\n",
    "\n",
    "| Par√°metro | Valor Ejemplo | Descripci√≥n |\n",
    "| :--- | :--- | :--- |\n",
    "| `latitude` / `longitude` | `40.416`, `-3.703` | Ubicaci√≥n exacta del estadio. |\n",
    "| `start_date` / `end_date` | `2024-09-15` | Limitamos la b√∫squeda a un solo d√≠a. |\n",
    "| `hourly` | `temperature_2m` | Temperatura a 2 metros del suelo. |\n",
    "| `hourly` | `apparent_temperature` | Sensaci√≥n t√©rmica (Heat Index / Wind Chill). |\n",
    "| `hourly` | `precipitation` | Suma de lluvia + nieve en mm. |\n",
    "| `hourly` | `wind_speed_10m` | Velocidad del viento a 10 metros de altura. |\n",
    "| `hourly` | `weather_code` | C√≥digo WMO (Num√©rico) que resume el clima. |\n",
    "| `timezone` | `Europe/Madrid` | **Cr√≠tico:** Asegura que las 18:00 sean hora local, no UTC. |\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è 4. L√≥gica Detallada paso a paso\n",
    "\n",
    "### üü¢ PASO 1: Configuraci√≥n de Robustez (Cach√© y Reintentos)\n",
    "\n",
    "Las peticiones HTTP pueden fallar por mil razones (micro-cortes de internet, servidor ocupado). Para evitar que el script se rompa a la mitad (ej: en el partido 200 de 380), implementamos dos capas de seguridad:\n",
    "\n",
    "1.  **`requests_cache`:**\n",
    "    * Crea un archivo oculto `.cache`.\n",
    "    * *Funci√≥n:* Si ejecutas el script dos veces, la segunda vez **no conecta a internet** para los datos que ya tiene. Los lee del disco. Esto ahorra tiempo y evita baneos.\n",
    "2.  **`retry_requests`:**\n",
    "    * Configurado con `retries=5` y `backoff_factor=0.2`.\n",
    "    * *Funci√≥n:* Si la API da error 500 o timeout, el script espera 0.2 segundos y reintenta. Si falla, espera 0.4s, luego 0.8s... as√≠ hasta 5 veces antes de rendirse.\n",
    "\n",
    "### üü° PASO 2: Procesamiento Temporal (La fecha ISO)\n",
    "\n",
    "El CSV original suele tener fechas en formato humano/europeo (`DD/MM/YYYY`, ej: `15/09/2024`), pero las APIs siempre exigen el est√°ndar internacional **ISO 8601** (`YYYY-MM-DD`).\n",
    "\n",
    "* **Transformaci√≥n:**\n",
    "    ```python\n",
    "    day, month, year = date_raw.split('/')\n",
    "    api_date = f\"{year}-{month}-{day}\" # Resultado: \"2024-09-15\"\n",
    "    ```\n",
    "\n",
    "### üü† PASO 3: Extracci√≥n Horaria (El \"Francotirador\")\n",
    "\n",
    "La API de Open-Meteo devuelve un array con **24 valores** (uno por cada hora del d√≠a solicitado: 00:00, 01:00... 23:00).\n",
    "\n",
    "* **El Reto:** No queremos la temperatura media del d√≠a, queremos la temperatura a la hora del partido.\n",
    "* **La L√≥gica:**\n",
    "    1.  Tomamos la hora del partido: `18:30`.\n",
    "    2.  Extraemos la hora entera: `18` (variable `match_hour`).\n",
    "    3.  Usamos ese n√∫mero como **√≠ndice** para acceder al array de la API.\n",
    "    * *Ejemplo:* `response.Hourly().Variables(0).ValuesAsNumpy()[18]` nos da la temperatura exacta a las 18:00.\n",
    "\n",
    "### üî¥ PASO 4: Manejo de Errores (Try/Except)\n",
    "\n",
    "Es posible que algunos partidos tengan datos err√≥neos (ej: coordenadas vac√≠as, fechas futuras que la API hist√≥rica no tiene).\n",
    "\n",
    "* **Bloque `try...except`:**\n",
    "    * Si ocurre un error en una fila espec√≠fica, el script **no se detiene**.\n",
    "    * Imprime un aviso en consola (`[ERROR] Fallo en fila X...`).\n",
    "    * Rellena los campos clim√°ticos con `None` (Vac√≠o) y salta al siguiente partido.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ 5. Resultados y Estructura de Salida\n",
    "\n",
    "El archivo generado, `partidos_con_clima_completo.csv`, mantiene todas las columnas originales y a√±ade 5 nuevas m√©tricas al final:\n",
    "\n",
    "| Nueva Columna | Unidad | Significado |\n",
    "| :--- | :--- | :--- |\n",
    "| **`Temperatura_C`** | Grados Celsius (¬∞C) | Temperatura real del aire. |\n",
    "| **`Sensacion_Termica_C`** | Grados Celsius (¬∞C) | C√≥mo se siente el cuerpo humano (humedad/viento). |\n",
    "| **`Precipitacion_mm`** | Mil√≠metros (mm) | Cantidad de lluvia en esa hora (0 = Seco). |\n",
    "| **`Viento_kmh`** | km/h | Velocidad media del viento. |\n",
    "| **`Codigo_Clima`** | WMO Code (0-99) | [Tabla de c√≥digos WMO](https://www.nodc.noaa.gov/archive/arc0021/0002199/1.1/data/0-data/HTML/WMO-CODE/WMO4677.HTM). <br>Ej: `0`=Despejado, `61`=Lluvia ligera, `95`=Tormenta. |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Dificultades T√©cnicas Superadas\n",
    "\n",
    "| Dificultad | Soluci√≥n T√©cnica |\n",
    "| :--- | :--- |\n",
    "| **L√≠mites de API** | Uso de cach√© local para evitar peticiones redundantes. |\n",
    "| **Inestabilidad de Red** | Implementaci√≥n de sistema de *Retries* (reintentos exponenciales). |\n",
    "| **Formato de Fecha** | Conversi√≥n manual de string `DD/MM/YYYY` a `YYYY-MM-DD`. |\n",
    "| **Precisi√≥n Horaria** | Indexaci√≥n directa del array Numpy devuelto por la API usando la hora del evento. |\n",
    "| **Datos Faltantes** | Verificaci√≥n previa de coordenadas (`pd.isna`) antes de llamar a la API. |\n",
    "| **Bloqueo SSL** | Uso de `urllib3.disable_warnings` para redes corporativas o Wi-Fi restringido. |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Conclusi√≥n\n",
    "\n",
    "Este script transforma un dataset est√°tico en uno din√°mico y contextual. Al cruzar coordenadas espaciales y temporales con bases de datos meteorol√≥gicas, habilitamos la capacidad de realizar **an√°lisis de factores externos** sobre el rendimiento deportivo y la afluencia de p√∫blico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "--- INICIO DEL PROCESO DE DATOS CLIM√ÅTICOS ---\n",
      "============================================\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 1: Cargando archivo de entrada\n",
      "-------------------------------------------------------------\n",
      "   [OK] Archivo cargado: hop.txt.csv\n",
      "   [INFO] Total de partidos a procesar: 380\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 2: Conectando con Open-Meteo para cada partido...\n",
      "-------------------------------------------------------------\n",
      "   > Procesado 20 / 380 partidos... (Fecha: 2024-08-25)\n",
      "   > Procesado 40 / 380 partidos... (Fecha: 2024-09-13)\n",
      "   > Procesado 60 / 380 partidos... (Fecha: 2024-09-22)\n",
      "   > Procesado 80 / 380 partidos... (Fecha: 2024-09-30)\n",
      "   > Procesado 100 / 380 partidos... (Fecha: 2024-10-21)\n",
      "   > Procesado 120 / 380 partidos... (Fecha: 2024-11-09)\n",
      "   > Procesado 140 / 380 partidos... (Fecha: 2024-11-30)\n",
      "   > Procesado 160 / 380 partidos... (Fecha: 2024-12-13)\n",
      "   > Procesado 180 / 380 partidos... (Fecha: 2024-12-22)\n",
      "   > Procesado 200 / 380 partidos... (Fecha: 2025-01-20)\n",
      "   > Procesado 220 / 380 partidos... (Fecha: 2025-02-03)\n",
      "   > Procesado 240 / 380 partidos... (Fecha: 2025-02-17)\n",
      "   > Procesado 260 / 380 partidos... (Fecha: 2025-03-08)\n",
      "   > Procesado 280 / 380 partidos... (Fecha: 2025-03-29)\n",
      "   > Procesado 300 / 380 partidos... (Fecha: 2025-04-11)\n",
      "   > Procesado 320 / 380 partidos... (Fecha: 2025-04-22)\n",
      "   > Procesado 340 / 380 partidos... (Fecha: 2025-05-05)\n",
      "   > Procesado 360 / 380 partidos... (Fecha: 2025-05-15)\n",
      "   > Procesado 380 / 380 partidos... (Fecha: 2025-05-25)\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 3: Guardando archivo enriquecido\n",
      "-------------------------------------------------------------\n",
      "‚úÖ PROCESO COMPLETADO EXITOSAMENTE.\n",
      "üìÇ Archivo guardado en: outputs/partidos_con_clima_completo.csv\n",
      "   Contiene nuevas columnas: Temperatura, Lluvia, Viento, etc.\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PROYECTO: ENRIQUECIMIENTO CLIM√ÅTICO DE PARTIDOS (HIST√ìRICO)\n",
    "# DESCRIPCI√ìN: Consulta la API de Open-Meteo para obtener temperatura, lluvia y viento\n",
    "#              en la fecha y hora exacta de cada partido pasado.\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. IMPORTACI√ìN DE LIBRER√çAS Y CONFIGURACI√ìN API\n",
    "# ==============================================================================\n",
    "# 'openmeteo_requests': Cliente oficial para conectar con la API del clima.\n",
    "# 'requests_cache': Para guardar respuestas en memoria y no pedir lo mismo dos veces.\n",
    "# 'retry_requests': Para reintentar autom√°ticamente si falla la conexi√≥n (internet inestable).\n",
    "# 'urllib3': Para gestionar advertencias de seguridad SSL.\n",
    "\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import urllib3\n",
    "\n",
    "print(\"============================================\")\n",
    "print(\"--- INICIO DEL PROCESO DE DATOS CLIM√ÅTICOS ---\")\n",
    "print(\"============================================\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONFIGURACI√ìN T√âCNICA\n",
    "# ---------------------------------------------------------\n",
    "# Desactivamos las advertencias de seguridad SSL (necesario a veces en redes corporativas)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Configuramos el sistema de cach√© y reintentos para la API\n",
    "# Esto hace que el script sea robusto: si la API falla un segundo, lo vuelve a intentar 5 veces.\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# URL del servicio de \"Archivo\" (Hist√≥rico) de Open-Meteo\n",
    "URL_API = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURACI√ìN DE RUTAS (IMPORTANTE: REVISAR ESTO)\n",
    "# ==============================================================================\n",
    "# Define aqu√≠ d√≥nde est√° tu archivo con los partidos Y las coordenadas.\n",
    "# EJEMPLO: Aseg√∫rate de que este CSV tenga columnas 'Latitud', 'Longitud', 'Date' y 'Time'.\n",
    "\n",
    "RUTA_ARCHIVO_ENTRADA = r\"inputs/hop.txt.csv\"\n",
    "\n",
    "# Carpeta donde guardaremos el resultado final con clima\n",
    "CARPETA_SALIDA = r\"outputs\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CARGA DE DATOS\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 1: Cargando archivo de entrada\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "if os.path.exists(RUTA_ARCHIVO_ENTRADA):\n",
    "    df = pd.read_csv(RUTA_ARCHIVO_ENTRADA)\n",
    "    print(f\"   [OK] Archivo cargado: {os.path.basename(RUTA_ARCHIVO_ENTRADA)}\")\n",
    "    print(f\"   [INFO] Total de partidos a procesar: {len(df)}\")\n",
    "    \n",
    "    # Verificaci√≥n r√°pida de columnas necesarias\n",
    "    cols_necesarias = ['Latitud', 'Longitud', 'Date', 'Time']\n",
    "    # Nota: Si tus columnas se llaman diferente (ej: 'Lat', 'Lon'), c√°mbialo abajo en el bucle.\n",
    "    \n",
    "else:\n",
    "    print(f\"   [ERROR CR√çTICO] No se encuentra el archivo: {RUTA_ARCHIVO_ENTRADA}\")\n",
    "    print(\"   Por favor, verifica la ruta en la secci√≥n 2 del c√≥digo.\")\n",
    "    exit() # Detiene el programa si no hay archivo\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PREPARACI√ìN DE LISTAS DE ALMACENAMIENTO\n",
    "# ==============================================================================\n",
    "# Aqu√≠ guardaremos los datos que nos vaya dando la API fila por fila\n",
    "temps = []      # Temperatura (2 metros sobre suelo)\n",
    "app_temps = []  # Sensaci√≥n t√©rmica\n",
    "precips = []    # Precipitaci√≥n (lluvia)\n",
    "winds = []      # Velocidad del viento\n",
    "codes = []      # C√≥digo del clima (0=Sol, 61=Lluvia, etc.)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. BUCLE MAESTRO: CONSULTA A LA API (FILA POR FILA)\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 2: Conectando con Open-Meteo para cada partido...\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        # --- A. VALIDACI√ìN PREVIA ---\n",
    "        # Si falta la latitud o longitud, no podemos pedir clima.\n",
    "        if pd.isna(row.get('Latitud')) or pd.isna(row.get('Longitud')):\n",
    "            print(f\"   [SALTADO] Fila {index}: Sin coordenadas.\")\n",
    "            temps.append(None); app_temps.append(None); precips.append(None); winds.append(None); codes.append(None)\n",
    "            continue\n",
    "\n",
    "        # --- B. EXTRACCI√ìN DE DATOS DE LA FILA ---\n",
    "        lat = row['Latitud']\n",
    "        lon = row['Longitud']\n",
    "        date_raw = row['Date']  # Se asume formato DD/MM/YYYY\n",
    "        time_raw = row['Time']  # Se asume formato HH:MM\n",
    "\n",
    "        # Convertir fecha de DD/MM/YYYY a YYYY-MM-DD (Formato ISO para la API)\n",
    "        day, month, year = date_raw.split('/')\n",
    "        api_date = f\"{year}-{month}-{day}\"\n",
    "\n",
    "        # Obtener la hora del partido redondeada (ej: 18:30 -> 18)\n",
    "        # Esto sirve para buscar el √≠ndice en el array de 24 horas que devuelve la API\n",
    "        match_hour = int(time_raw.split(':')[0])\n",
    "\n",
    "        # --- C. CONFIGURACI√ìN DE LA PETICI√ìN ---\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": api_date,\n",
    "            \"end_date\": api_date,\n",
    "            \"hourly\": [\n",
    "                \"temperature_2m\",         # Temp real\n",
    "                \"apparent_temperature\",   # Sensaci√≥n\n",
    "                \"precipitation\",          # Lluvia\n",
    "                \"wind_speed_10m\",         # Viento\n",
    "                \"weather_code\"            # Icono/Resumen\n",
    "            ],\n",
    "            \"timezone\": \"Europe/Madrid\"   # Importante para ajustar la hora\n",
    "        }\n",
    "\n",
    "        # --- D. LLAMADA A LA API ---\n",
    "        # verify=False evita errores de SSL en algunas redes wifi\n",
    "        responses = openmeteo.weather_api(URL_API, params=params, verify=False)\n",
    "\n",
    "        # Procesamos la respuesta (La API devuelve un objeto complejo)\n",
    "        response = responses[0]\n",
    "        hourly = response.Hourly()\n",
    "\n",
    "        # --- E. EXTRACCI√ìN EXACTA DE LA HORA DEL PARTIDO ---\n",
    "        # La API devuelve 24 datos (uno por hora del d√≠a).\n",
    "        # Usamos 'match_hour' para coger solo el dato de la hora del pitido inicial.\n",
    "        t2m = hourly.Variables(0).ValuesAsNumpy()[match_hour]\n",
    "        att = hourly.Variables(1).ValuesAsNumpy()[match_hour]\n",
    "        prc = hourly.Variables(2).ValuesAsNumpy()[match_hour]\n",
    "        wnd = hourly.Variables(3).ValuesAsNumpy()[match_hour]\n",
    "        wcd = hourly.Variables(4).ValuesAsNumpy()[match_hour]\n",
    "\n",
    "        # Guardamos en nuestras listas\n",
    "        temps.append(t2m)\n",
    "        app_temps.append(att)\n",
    "        precips.append(prc)\n",
    "        winds.append(wnd)\n",
    "        codes.append(wcd)\n",
    "\n",
    "        # --- F. FEEDBACK EN PANTALLA ---\n",
    "        # Mostramos progreso cada 20 filas para no saturar la consola\n",
    "        if (index + 1) % 20 == 0:\n",
    "            print(f\"   > Procesado {index + 1} / {len(df)} partidos... (Fecha: {api_date})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Si algo falla (fecha futura, error de formato, etc.), rellenamos con vac√≠o\n",
    "        print(f\"   [ERROR] Fallo en fila {index} ({date_raw}): {e}\")\n",
    "        temps.append(None)\n",
    "        app_temps.append(None)\n",
    "        precips.append(None)\n",
    "        winds.append(None)\n",
    "        codes.append(None)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. GUARDADO DE RESULTADOS\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 3: Guardando archivo enriquecido\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "# A√±adimos las listas como nuevas columnas al DataFrame original\n",
    "df['Temperatura_C'] = temps\n",
    "df['Sensacion_Termica_C'] = app_temps\n",
    "df['Precipitacion_mm'] = precips\n",
    "df['Viento_kmh'] = winds\n",
    "df['Codigo_Clima'] = codes\n",
    "\n",
    "# Definimos la ruta de salida\n",
    "ruta_salida_final = os.path.join(CARPETA_SALIDA, \"partidos_con_clima_completo.csv\")\n",
    "\n",
    "# Guardamos el CSV\n",
    "df.to_csv(ruta_salida_final, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"‚úÖ PROCESO COMPLETADO EXITOSAMENTE.\")\n",
    "print(f\"üìÇ Archivo guardado en: {ruta_salida_final}\")\n",
    "print(\"   Contiene nuevas columnas: Temperatura, Lluvia, Viento, etc.\")\n",
    "print(\"=============================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPE - Medici√≥n de Expectaci√≥n P√∫blica (Google Trends)\n",
    "\n",
    "### Objetivo\n",
    "Esta secci√≥n cuantifica el **nivel de inter√©s p√∫blico** que genera cada partido utilizando datos de b√∫squeda de Google Trends. El objetivo es a√±adir una m√©trica que capture la \"expectaci√≥n\" o \"hype\" del enfrentamiento.\n",
    "\n",
    "### Metodolog√≠a\n",
    "\n",
    "#### 1. **Fuente de datos**: Google Trends API (`pytrends`)\n",
    "- Consulta b√∫squedas realizadas en **Espa√±a** (`geo='ES'`)\n",
    "- Idioma: Espa√±ol (`hl='es-ES'`)\n",
    "\n",
    "#### 2. **T√©rmino de b√∫squeda** (MEJORADO):\n",
    "- **Enfoque anterior**: Buscaba `\"Equipo Local vs Equipo Visitante\"` (muy espec√≠fico, pocas b√∫squedas)\n",
    "- **Enfoque mejorado**: Busca nombres de **equipos individuales** y calcula el promedio\n",
    "  - Ejemplo: Para Real Madrid vs Barcelona\n",
    "    - Busca: `\"Real Madrid\"` ‚Üí valor m√°ximo: 85\n",
    "    - Busca: `\"Barcelona\"` ‚Üí valor m√°ximo: 92\n",
    "    - Hype del partido = (85 + 92) / 2 = **88.5**\n",
    "\n",
    "#### 3. **Ventana temporal**:\n",
    "- **7 d√≠as** alrededor del partido:\n",
    "  - 3 d√≠as antes\n",
    "  - D√≠a del partido  \n",
    "  - 3 d√≠as despu√©s\n",
    "- Esto captura el pico de inter√©s antes, durante y despu√©s del evento\n",
    "- Se toma el **valor m√°ximo** de cada equipo en esa ventana\n",
    "\n",
    "#### 4. **Normalizaci√≥n global** (POST-PROCESAMIENTO):\n",
    "Problema detectado: Google Trends normaliza valores dentro de cada consulta individual, lo que genera solo valores 0 o 100.\n",
    "\n",
    "**Soluci√≥n aplicada**:\n",
    "- Despu√©s de obtener todos los valores brutos, se aplica **normalizaci√≥n Min-Max** sobre todo el dataset:\n",
    "  - `Hype_normalizado = ((Hype - Hype_min) / (Hype_max - Hype_min)) √ó 100`\n",
    "- Esto garantiza una **distribuci√≥n continua** de valores entre 0 y 100\n",
    "\n",
    "#### 5. **Protecci√≥n anti-bloqueo**:\n",
    "Google Trends tiene l√≠mites estrictos de consultas:\n",
    "- Pausa aleatoria de **5-10 segundos** entre peticiones\n",
    "- Si detecta error 429 (Too Many Requests), pausa de **60 segundos**\n",
    "- Proceso completo puede tardar **30-60 minutos** para 380 partidos\n",
    "\n",
    "### Resultado\n",
    "Se a√±ade la columna **`Hype_Google_Trends`** al dataset con valores 0-100 (normalizados globalmente) que representan el nivel de expectaci√≥n p√∫blica de cada partido.\n",
    "\n",
    "**Archivo generado**: `outputs/partidos_completo_con_hype.csv`\n",
    "\n",
    "### Interpretaci√≥n\n",
    "- **Hype alto (>70)**: Partidos muy esperados (Real Madrid, Barcelona, Atl√©tico Madrid, etc.)\n",
    "- **Hype medio (30-70)**: Partidos con inter√©s moderado\n",
    "- **Hype bajo (<30)**: Partidos con poco seguimiento medi√°tico\n",
    "\n",
    "### Ventajas de la metodolog√≠a mejorada\n",
    "‚úÖ **M√°s datos**: Buscar equipos individuales captura m√°s b√∫squedas que \"Local vs Visitante\"  \n",
    "‚úÖ **Distribuci√≥n continua**: La normalizaci√≥n global elimina el problema de valores binarios (0 o 100)  \n",
    "‚úÖ **Valores comparables**: Todos los partidos est√°n en la misma escala relativa  \n",
    "‚úÖ **Refleja popularidad**: El promedio de b√∫squedas de ambos equipos es un buen proxy del inter√©s del partido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "--- INICIO DEL AN√ÅLISIS DE HYPE (GOOGLE TRENDS) ---\n",
      "============================================\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 1: Cargando datos de partidos\n",
      "-------------------------------------------------------------\n",
      "   [OK] Archivo cargado. Total partidos: 380\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 2: Consultando Google Trends (Esto puede tardar...)\n",
      "-------------------------------------------------------------\n",
      "   > 5/380 | CA Osasuna           vs CD Legan√©s           -> Hype: 50.0\n",
      "   > 10/380 | Villarreal CF        vs Atl√©tico de Madrid   -> Hype: 53.0\n",
      "   > 15/380 | Getafe CF            vs Rayo Vallecano       -> Hype: 51.0\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 20/380 | Atl√©tico de Madrid   vs Girona FC            -> Hype: 71.5\n",
      "   > 25/380 | Real Valladolid      vs CD Legan√©s           -> Hype: 50.0\n",
      "   > 30/380 | FC Barcelona         vs Real Valladolid      -> Hype: 74.5\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 35/380 | Deportivo Alav√©s     vs UD Las Palmas        -> Hype: 68.5\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 40/380 | Real Betis           vs CD Legan√©s           -> Hype: 0.0\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 45/380 | RC Celta             vs Real Valladolid      -> Hype: 52.0\n",
      "   > 50/380 | RCD Mallorca         vs Real Sociedad        -> Hype: 51.0\n",
      "   > 55/380 | CA Osasuna           vs UD Las Palmas        -> Hype: 53.5\n",
      "   > 60/380 | Villarreal CF        vs FC Barcelona         -> Hype: 52.5\n",
      "   > 65/380 | Real Madrid          vs Deportivo Alav√©s     -> Hype: 50.5\n",
      "   > 70/380 | RC Celta             vs Atl√©tico de Madrid   -> Hype: 50.5\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 75/380 | CA Osasuna           vs FC Barcelona         -> Hype: 51.0\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 80/380 | Villarreal CF        vs UD Las Palmas        -> Hype: 71.5\n",
      "   > 85/380 | Real Valladolid      vs Rayo Vallecano       -> Hype: 68.0\n",
      "   > 90/380 | Real Sociedad        vs Atl√©tico de Madrid   -> Hype: 62.0\n",
      "   > 95/380 | RC Celta             vs Real Madrid          -> Hype: 50.0\n",
      "   > 100/380 | Valencia CF          vs UD Las Palmas        -> Hype: 66.0\n",
      "   > 105/380 | Real Madrid          vs FC Barcelona         -> Hype: 53.5\n",
      "   > 110/380 | RCD Mallorca         vs Athletic Club        -> Hype: 60.5\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 115/380 | FC Barcelona         vs RCD Espanyol         -> Hype: 60.5\n",
      "   > 120/380 | Real Madrid          vs CA Osasuna           -> Hype: 50.0\n",
      "   > 125/380 | Getafe CF            vs Girona FC            -> Hype: 59.0\n",
      "   > 130/380 | Atl√©tico de Madrid   vs Deportivo Alav√©s     -> Hype: 53.5\n",
      "   > 135/380 | Sevilla FC           vs Rayo Vallecano       -> Hype: 72.5\n",
      "   > 140/380 | Deportivo Alav√©s     vs CD Legan√©s           -> Hype: 50.0\n",
      "   > 145/380 | Rayo Vallecano       vs Athletic Club        -> Hype: 67.0\n",
      "   > 150/380 | RC Celta             vs RCD Mallorca         -> Hype: 60.5\n",
      "   > 155/380 | CD Legan√©s           vs Real Sociedad        -> Hype: 50.0\n",
      "   > 160/380 | Real Valladolid      vs Valencia CF          -> Hype: 84.0\n",
      "   > 165/380 | Atl√©tico de Madrid   vs Getafe CF            -> Hype: 54.5\n",
      "   > 170/380 | Villarreal CF        vs Rayo Vallecano       -> Hype: 52.0\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 175/380 | CA Osasuna           vs Athletic Club        -> Hype: 50.0\n",
      "   > 180/380 | CD Legan√©s           vs Villarreal CF        -> Hype: 88.5\n",
      "   > 185/380 | Real Valladolid      vs Real Betis           -> Hype: 88.0\n",
      "   > 190/380 | Real Sociedad        vs Villarreal CF        -> Hype: 50.5\n",
      "   > 195/380 | Getafe CF            vs FC Barcelona         -> Hype: 58.0\n",
      "   > 200/380 | Villarreal CF        vs RCD Mallorca         -> Hype: 99.0\n",
      "   > 205/380 | Real Valladolid      vs Real Madrid          -> Hype: 74.5\n",
      "   > 210/380 | Deportivo Alav√©s     vs RC Celta             -> Hype: 61.0\n",
      "   > 215/380 | RCD Espanyol         vs Real Madrid          -> Hype: 68.5\n",
      "   > 220/380 | Girona FC            vs UD Las Palmas        -> Hype: 76.5\n",
      "   > 225/380 | Real Madrid          vs Atl√©tico de Madrid   -> Hype: 55.5\n",
      "   > 230/380 | RCD Mallorca         vs CA Osasuna           -> Hype: 50.0\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 235/380 | Villarreal CF        vs Valencia CF          -> Hype: 0.0\n",
      "   > 240/380 | FC Barcelona         vs Rayo Vallecano       -> Hype: 58.5\n",
      "   > 245/380 | UD Las Palmas        vs FC Barcelona         -> Hype: 63.0\n",
      "   > 250/380 | Sevilla FC           vs RCD Mallorca         -> Hype: 52.5\n",
      "   > 255/380 | Atl√©tico de Madrid   vs Athletic Club        -> Hype: 60.0\n",
      "   > 260/380 | RC Celta             vs CD Legan√©s           -> Hype: 92.5\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 265/380 | Real Betis           vs UD Las Palmas        -> Hype: 64.5\n",
      "   > 270/380 | Real Valladolid      vs RC Celta             -> Hype: 52.0\n",
      "   > 275/380 | Sevilla FC           vs Athletic Club        -> Hype: 94.5\n",
      "   > 280/380 | Real Sociedad        vs Real Valladolid      -> Hype: 59.0\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 285/380 | FC Barcelona         vs Girona FC            -> Hype: 65.5\n",
      "   > 290/380 | Rayo Vallecano       vs RCD Espanyol         -> Hype: 87.0\n",
      "   > 295/380 | UD Las Palmas        vs Real Sociedad        -> Hype: 55.0\n",
      "   > 300/380 | Valencia CF          vs Sevilla FC           -> Hype: 90.5\n",
      "   > 305/380 | CA Osasuna           vs Girona FC            -> Hype: 50.0\n",
      "   > 310/380 | RCD Espanyol         vs Getafe CF            -> Hype: 51.5\n",
      "   > 315/380 | Real Valladolid      vs CA Osasuna           -> Hype: 50.0\n",
      "   > 320/380 | Valencia CF          vs RCD Espanyol         -> Hype: 64.0\n",
      "   > 325/380 | Deportivo Alav√©s     vs Real Sociedad        -> Hype: 51.0\n",
      "   > 330/380 | Villarreal CF        vs RCD Espanyol         -> Hype: 51.0\n",
      "   > 335/380 | Real Valladolid      vs FC Barcelona         -> Hype: 58.0\n",
      "   > 340/380 | Girona FC            vs RCD Mallorca         -> Hype: 54.0\n",
      "   > 345/380 | Girona FC            vs Villarreal CF        -> Hype: 54.0\n",
      "   > 350/380 | Real Betis           vs CA Osasuna           -> Hype: 50.0\n",
      "   > 355/380 | Deportivo Alav√©s     vs Valencia CF          -> Hype: 58.5\n",
      "   > 360/380 | RCD Espanyol         vs FC Barcelona         -> Hype: 62.0\n",
      "   > 365/380 | UD Las Palmas        vs CD Legan√©s           -> Hype: 50.0\n",
      "   > 370/380 | Sevilla FC           vs Real Madrid          -> Hype: 54.5\n",
      "   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\n",
      "   > 375/380 | Getafe CF            vs RC Celta             -> Hype: 77.0\n",
      "   > 380/380 | Athletic Club        vs FC Barcelona         -> Hype: 78.0\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 3: Normalizando valores de Hype globalmente\n",
      "-------------------------------------------------------------\n",
      "   Valor m√≠nimo detectado: 0.00\n",
      "   Valor m√°ximo detectado: 99.50\n",
      "   ‚úÖ Normalizaci√≥n aplicada correctamente\n",
      "\n",
      "-------------------------------------------------------------\n",
      "--> PASO 4: Guardando resultados\n",
      "-------------------------------------------------------------\n",
      "‚úÖ PROCESO COMPLETADO.\n",
      "üìÇ Archivo guardado: outputs/partidos_completo_con_hype.csv\n",
      "   Nueva columna a√±adida: 'Hype_Google_Trends' (Escala 0-100, normalizada)\n",
      "   Estad√≠sticas finales:\n",
      "      - Media: 60.31\n",
      "      - Mediana: 55.78\n",
      "      - Desviaci√≥n est√°ndar: 18.71\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PROYECTO: MEDICI√ìN DE HYPE (GOOGLE TRENDS) - LALIGA 2024-25\n",
    "# DESCRIPCI√ìN: Consulta Google Trends para obtener un √≠ndice de inter√©s (0-100)\n",
    "#              para cada enfrentamiento bas√°ndose en b√∫squedas de equipos.\n",
    "# MEJORA: Busca equipos individuales + normalizaci√≥n global post-procesamiento\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. IMPORTACI√ìN DE LIBRER√çAS\n",
    "# ==============================================================================\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "print(\"============================================\")\n",
    "print(\"--- INICIO DEL AN√ÅLISIS DE HYPE (GOOGLE TRENDS) ---\")\n",
    "print(\"============================================\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURACI√ìN\n",
    "# ==============================================================================\n",
    "# Configura la conexi√≥n con Google.\n",
    "# hl='es-ES': Idioma espa√±ol.\n",
    "# tz=360: Zona horaria (minutos), aunque no es cr√≠tico para datos diarios.\n",
    "pytrends = TrendReq(hl='es-ES', tz=360)\n",
    "\n",
    "# RUTAS (AJUSTA ESTO A TU ORDENADOR)\n",
    "# Usaremos el archivo que ya tiene clima o el de partidos base.\n",
    "RUTA_ENTRADA = r\"outputs/partidos_con_clima_completo.csv\"\n",
    "CARPETA_SALIDA = r\"outputs\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CARGA DE DATOS\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 1: Cargando datos de partidos\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "if os.path.exists(RUTA_ENTRADA):\n",
    "    df = pd.read_csv(RUTA_ENTRADA)\n",
    "    print(f\"   [OK] Archivo cargado. Total partidos: {len(df)}\")\n",
    "else:\n",
    "    print(f\"   [ERROR] No se encuentra: {RUTA_ENTRADA}\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. FUNCI√ìN AUXILIAR: OBTENER HYPE (MEJORADA)\n",
    "# ==============================================================================\n",
    "def obtener_hype_google(equipo_local, equipo_visitante, fecha_partido):\n",
    "    \"\"\"\n",
    "    Consulta Google Trends para los nombres de ambos equipos en una ventana de 7 d√≠as \n",
    "    alrededor del partido. Devuelve el promedio de inter√©s de ambos equipos.\n",
    "    \n",
    "    MEJORA: En lugar de buscar \"Local vs Visitante\" (b√∫squeda muy espec√≠fica),\n",
    "    busca los nombres de cada equipo individualmente para obtener m√°s datos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Construir la b√∫squeda: Nombres de equipos individuales\n",
    "        # Esto captura m√°s b√∫squedas que \"Equipo1 vs Equipo2\"\n",
    "        kw_list = [equipo_local, equipo_visitante]\n",
    "        \n",
    "        # 2. Definir ventana de tiempo (3 d√≠as antes y 3 d√≠as despu√©s para capturar el pico)\n",
    "        # Formato fecha entrada: DD/MM/YYYY -> Convertir a datetime\n",
    "        fecha_obj = datetime.strptime(fecha_partido, \"%d/%m/%Y\")\n",
    "        start_date = (fecha_obj - timedelta(days=3)).strftime(\"%Y-%m-%d\")\n",
    "        end_date = (fecha_obj + timedelta(days=3)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        timeframe = f\"{start_date} {end_date}\"\n",
    "        \n",
    "        # 3. Petici√≥n a Google\n",
    "        # geo='ES': Solo b√∫squedas en Espa√±a\n",
    "        pytrends.build_payload(kw_list, cat=0, timeframe=timeframe, geo='ES', gprop='')\n",
    "        \n",
    "        # 4. Obtener datos de inter√©s a lo largo del tiempo\n",
    "        data = pytrends.interest_over_time()\n",
    "        \n",
    "        if not data.empty:\n",
    "            # Extraer valores m√°ximos para cada equipo\n",
    "            hype_local = data[equipo_local].max() if equipo_local in data.columns else 0\n",
    "            hype_visitante = data[equipo_visitante].max() if equipo_visitante in data.columns else 0\n",
    "            \n",
    "            # Promedio de ambos equipos como proxy de inter√©s total del partido\n",
    "            hype_score = (hype_local + hype_visitante) / 2\n",
    "            return hype_score\n",
    "        else:\n",
    "            return 0 # Si no hay datos suficientes\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Google a veces bloquea si hacemos muchas peticiones seguidas (Error 429)\n",
    "        if \"429\" in str(e):\n",
    "            print(\"   [ALERTA] Bloqueo de Google (Too Many Requests). Esperando...\")\n",
    "            time.sleep(60) # Pausa larga si nos bloquean\n",
    "        return 0  # Retorna 0 en lugar de None para evitar problemas\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. BUCLE PRINCIPAL\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 2: Consultando Google Trends (Esto puede tardar...)\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "hypes = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    local = row['Local']     # O 'HomeTeam' seg√∫n tu CSV\n",
    "    visitante = row['Visitante'] # O 'AwayTeam'\n",
    "    fecha = row['Date']\n",
    "    \n",
    "    # Llamada a la funci√≥n\n",
    "    hype = obtener_hype_google(local, visitante, fecha)\n",
    "    hypes.append(hype)\n",
    "    \n",
    "    # LOG EN PANTALLA\n",
    "    # Mostramos barra de progreso\n",
    "    if (index + 1) % 5 == 0: # Actualiza cada 5 partidos\n",
    "        print(f\"   > {index + 1}/{len(df)} | {local:20s} vs {visitante:20s} -> Hype: {hype:.1f}\")\n",
    "    \n",
    "    # PAUSA ANTI-BLOQUEO (MUY IMPORTANTE)\n",
    "    # Google Trends es muy estricto. Necesitamos pausas aleatorias entre peticiones.\n",
    "    sleep_time = random.randint(5, 10) \n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. NORMALIZACI√ìN GLOBAL (POST-PROCESAMIENTO)\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 3: Normalizando valores de Hype globalmente\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "if hypes:\n",
    "    # Convierte a numpy array para facilitar operaciones\n",
    "    hypes_array = np.array(hypes)\n",
    "    \n",
    "    # Normalizaci√≥n Min-Max al rango 0-100 considerando TODOS los partidos\n",
    "    min_hype = hypes_array.min()\n",
    "    max_hype = hypes_array.max()\n",
    "    \n",
    "    print(f\"   Valor m√≠nimo detectado: {min_hype:.2f}\")\n",
    "    print(f\"   Valor m√°ximo detectado: {max_hype:.2f}\")\n",
    "    \n",
    "    if max_hype > min_hype:\n",
    "        # Aplica normalizaci√≥n Min-Max: (x - min) / (max - min) * 100\n",
    "        hypes_normalizados = ((hypes_array - min_hype) / (max_hype - min_hype)) * 100\n",
    "        print(f\"   ‚úÖ Normalizaci√≥n aplicada correctamente\")\n",
    "    else:\n",
    "        # Si todos los valores son iguales, no normalizar\n",
    "        hypes_normalizados = hypes_array\n",
    "        print(f\"   ‚ö†Ô∏è  Todos los valores son iguales, no se aplic√≥ normalizaci√≥n\")\n",
    "    \n",
    "    df['Hype_Google_Trends'] = hypes_normalizados\n",
    "else:\n",
    "    df['Hype_Google_Trends'] = hypes\n",
    "    print(f\"   ‚ö†Ô∏è  No hay valores de hype para normalizar\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. GUARDADO FINAL\n",
    "# ==============================================================================\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"--> PASO 4: Guardando resultados\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "ruta_final = os.path.join(CARPETA_SALIDA, \"partidos_completo_con_hype.csv\")\n",
    "df.to_csv(ruta_final, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"‚úÖ PROCESO COMPLETADO.\")\n",
    "print(f\"üìÇ Archivo guardado: {ruta_final}\")\n",
    "print(f\"   Nueva columna a√±adida: 'Hype_Google_Trends' (Escala 0-100, normalizada)\")\n",
    "print(f\"   Estad√≠sticas finales:\")\n",
    "print(f\"      - Media: {df['Hype_Google_Trends'].mean():.2f}\")\n",
    "print(f\"      - Mediana: {df['Hype_Google_Trends'].median():.2f}\")\n",
    "print(f\"      - Desviaci√≥n est√°ndar: {df['Hype_Google_Trends'].std():.2f}\")\n",
    "print(\"=============================================================\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
